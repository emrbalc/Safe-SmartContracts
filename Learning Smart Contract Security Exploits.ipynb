{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Learning of Smart Contract Security Exploits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Input data files are available in the \"./input_data/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"./input_data/final\"))\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pandas.tools.plotting import table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(df):    \n",
    "    # label data\n",
    "    df['LABEL'] = 0\n",
    "    df.loc[df['CATEGORY'] == '1 0 0 0', 'LABEL'] = 0\n",
    "    df.loc[df['CATEGORY'] != '1 0 0 0', 'LABEL'] = 1\n",
    "    \n",
    "def nlp_preprocess(df):\n",
    "    n_most_common_opcodes = 1000 #8000\n",
    "    max_len = 130\n",
    "\n",
    "    # Class Tokenizer - This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary)\n",
    "    tokenizer = Tokenizer(num_words=n_most_common_opcodes, lower=False)\n",
    "\n",
    "    # fit_on_texts - Updates internal vocabulary based on a list of texts. In the case where texts contains lists, we assume each entry of the lists to be a token.\n",
    "    tokenizer.fit_on_texts(df['OPCODE'].values)\n",
    "\n",
    "    # Transforms each text in texts in a sequence of integers.\n",
    "    sequences = tokenizer.texts_to_sequences(df['OPCODE'].values)\n",
    "\n",
    "    #Find number of unique tokens\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    #pad sequences with zeros in front to make them all maxlen\n",
    "    X = pad_sequences(sequences, maxlen=max_len)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def dftoXY(df):\n",
    "    # Save test X and y\n",
    "    X_test = nlp_preprocess(df)\n",
    "    # label data\n",
    "    label(df)\n",
    "    print(pd.value_counts(df['LABEL']))\n",
    "    y_test = to_categorical(df['LABEL'], num_classes=2)\n",
    "    \n",
    "    return X_test, y_test    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'clean_train.csv'\n",
    "data = pd.read_csv('./input_data/final/'+dataset, usecols=['ADDRESS', 'OPCODE', 'CATEGORY'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(data['CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading previously saved model\n",
    "from keras.models import load_model\n",
    "model = load_model('./saved_model/'+'9x_450k.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data[data['CATEGORY'] == '1 0 0 0'] # no vulnerabilities\n",
    "s = data[data['CATEGORY'] == '0 1 0 0'] # suicidal\n",
    "p = data[data['CATEGORY'] == '0 0 1 0'] # prodigal\n",
    "g = data[data['CATEGORY'] == '0 0 0 1'] # greedy\n",
    "sp = data[data['CATEGORY'] == '0 1 1 0'] # suicidal and prodigal\n",
    "\n",
    "# shuffle non-vulnerable set \n",
    "n_shuf = n.sample(frac=1, random_state=39, replace=False)\n",
    "\n",
    "# split non-vulnerable data \n",
    "num_train = 450000 \n",
    "num_test = len(n_shuf)-num_train\n",
    "norm_train = n_shuf.iloc[0:num_train]\n",
    "norm_test = n_shuf.iloc[num_train:]\n",
    "\n",
    "n_shuf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase vulnerable set by 9 times\n",
    "multiple = 9 \n",
    "pos = pd.concat([s,p,g,sp] * multiple) #8640\n",
    "\n",
    "# concatenate vulnerable and non-vulnerable into one set\n",
    "resampled_set = pd.concat([pos,norm_train], ignore_index=True)\n",
    "\n",
    "# shuffle entire set\n",
    "resampled_set = resampled_set.sample(frac=1, random_state=39, replace=False)\n",
    "pd.value_counts(resampled_set['CATEGORY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count resampled labels\n",
    "label(resampled_set)\n",
    "pd.value_counts(resampled_set['LABEL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dftoXY(resampled_set)\n",
    "\n",
    "# split processed dataset into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 0)\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "epochs = 50 \n",
    "emb_dim = 128 \n",
    "batch_size = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "n_most_common_opcodes = 1000\n",
    "\n",
    "model = Sequential()\n",
    "# n_most_common_opcodes=Size of the directory, emb_dim=Dimension of the dense embedding, input_length=Length of input sequences, when it is constant\n",
    "model.add(Embedding(n_most_common_opcodes, emb_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.85))\n",
    "model.add(LSTM(64, dropout=0.85, recurrent_dropout=0.85))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# training the model\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2)\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time taken: ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test FPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and reading csv input data \n",
    "leakFP = 'clean_test_leakFP_noDups.csv'\n",
    "suicidalFP = 'clean_test_suicidalFP_noDups.csv'\n",
    "greedyFP = 'clean_test_greedyFP_noDups.csv'\n",
    "\n",
    "leakFP_test = pd.read_csv('./input_data/'+leakFP, usecols=['ADDRESS', 'OPCODE', 'CATEGORY'])\n",
    "suicidalFP_test = pd.read_csv('./input_data/'+suicidalFP, usecols=['ADDRESS', 'OPCODE', 'CATEGORY'])\n",
    "greedyFP_test = pd.read_csv('./input_data/'+greedyFP, usecols=['ADDRESS', 'OPCODE', 'CATEGORY'])\n",
    "\n",
    "# evaluation of FPs\n",
    "X_leakFP, y_leakFP = dftoXY(leakFP_test)\n",
    "leakFP_accr = model.evaluate(X_leakFP,y_leakFP)\n",
    "print('Leak test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(leakFP_accr[0],leakFP_accr[1]))\n",
    "\n",
    "X_suicidalFP, y_suicidalFP = dftoXY(suicidalFP_test)\n",
    "suicidalFP_accr = model.evaluate(X_suicidalFP,y_suicidalFP)\n",
    "print('Suicidal test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(suicidalFP_accr[0],suicidalFP_accr[1]))\n",
    "\n",
    "X_greedyFP, y_greedyFP = dftoXY(greedyFP_test)\n",
    "greedyFP_accr = model.evaluate(X_greedyFP,y_greedyFP)\n",
    "print('Greedy test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(greedyFP_accr[0],greedyFP_accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Sample 10,000 from non-vulnerable set not previously used for training or testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10,000 contracts\n",
    "num_tr = 10000\n",
    "norm_test = norm_test.sample(num_tr, random_state=9, replace=False)\n",
    "\n",
    "# shuffle these 10,000 samples\n",
    "trtestset = pd.concat([norm_test], ignore_index=True)\n",
    "trtestset = trtestset.sample(frac=1, random_state=39, replace=False)\n",
    "\n",
    "# preprocess data\n",
    "X_trtest, y_trtest = dftoXY(trtestset) \n",
    "\n",
    "# evaluate model on the sample\n",
    "trtest_accr = model.evaluate(X_trtest,y_trtest)\n",
    "print('Test accuracy on set of unseen negatives: {:0.4f}\\n  Accuracy: {:0.4f}'.format(trtest_accr[0],trtest_accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and saving flagged negatives for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_neg = model.predict(X_trtest, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_neg[:,1])\n",
    "\n",
    "predneg = np.count_nonzero(pred_neg[:,1] > 0.5)\n",
    "predneg_accr = predneg / len(pred_neg)\n",
    "print('Percentage of Negatives Flagged: {:0.4f}'.format(predneg_accr))\n",
    "\n",
    "LSTM_pos = np.nonzero(pred_neg[:,1] > 0.5)\n",
    "np.array(np.nonzero(pred_neg[:,1] > 0.5)).shape\n",
    "\n",
    "lstm_Negflagged = norm_test.iloc[LSTM_pos]\n",
    "# lstm_Negflagged.to_csv ('./output_data/lstm_flaggedNeg_final.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
